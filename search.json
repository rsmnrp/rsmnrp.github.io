[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome",
    "section": "",
    "text": "Hi, nice to meet you! üçÄ\nI‚Äôm Qistina, a final-year Economics and Data Science student at Singapore Management University.\nI‚Äôm passionate about using data to explore topics in environmental sustainability, space conservation, and animal welfare. Since I was young, I‚Äôve been drawn to protecting animals and the planet. Today, I aim to use my skills to contribute meaningfully to these causes.\nWhen I‚Äôm not coding or analyzing data, I enjoy spending time with my cats üêà, watching movies üçø, and wandering around Singapore.\nI‚Äôm currently open to data-related opportunities‚Äîwhether it‚Äôs in analytics, research, or sustainability.\nLet‚Äôs connect!"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This portfolio is a space where I document what I‚Äôve learned, through both coding projects and blog posts.\nMy projects often involve data cleaning, visualization, and analysis using languages like Python and R.\nThis blog serves as a place where I break down complex topics in economics and finance into straightforward explanations. Writing allows me to clarify my understanding, and I hope these posts offer helpful insights to others as well."
  },
  {
    "objectID": "project_1.html",
    "href": "project_1.html",
    "title": "Big Data Analytics Group Project",
    "section": "",
    "text": "For our big data project, my group and I analysed the used car market dynamics in Singapore by looking at the factors that affect the price of a used car and their significance."
  },
  {
    "objectID": "project_3.html",
    "href": "project_3.html",
    "title": "HDB Resale Price Forecast",
    "section": "",
    "text": "For my group project, I forecasted HDB resale prices beyond 2025 using Tableau as well as created an overview of my group‚Äôs visualizations on Tableau Public."
  },
  {
    "objectID": "project_2.html",
    "href": "project_2.html",
    "title": "Time Series Data Analysis Project",
    "section": "",
    "text": "In this project, my group and I aim to investigate the relationship between climate change and three variables (greenhouse gas emissions, mean sea level and anomalies in land-surface air and sea-surface water temperatures) by examining its patterns and trends.\nThe dataset used encompasses global, yearly records from 1880 to 2020. We have also conducted forecasts for these variables over the next 80 years, providing insights into the climate change issue in the foreseeable future."
  },
  {
    "objectID": "project_1.html#final-dataset-features",
    "href": "project_1.html#final-dataset-features",
    "title": "Big Data Analytics Group Project",
    "section": "Final Dataset Features",
    "text": "Final Dataset Features\n\nCURB_WEIGHT_KG\nOMV\nCOE_LISTED:\nENGINE_CAPACITY_CC\nDAYS_OF_COE_LEFT\nMILEAGE_KM\nFEATURE_COUNT\nACCESSORIES_COUNT\nAGE_SINCE_MANUFACTURED\nNO_OF_OWNERS\nDAYS_SINCE_REGISTERED\nTYPE\nTRANSMISSION\nBRAND_CATEGORY\n\nWe then plot correlation plots, raster plots and bar plots with the final variables."
  },
  {
    "objectID": "project_1.html#correlation-plots-between-final-variables",
    "href": "project_1.html#correlation-plots-between-final-variables",
    "title": "Big Data Analytics Group Project",
    "section": "Correlation Plots Between Final Variables",
    "text": "Correlation Plots Between Final Variables\nWe see that variables CURB_WEIGHT_KG, OMV, COE_LISTED, ENGINE_CAPACITY_CC, DAYS_OF_COE_LEFT have a positive relationship with PRICE while variables MILEAGE_KM, FEATURE_COUNT, ACCESSORIES_COUNT, AGE_SINCE_MANUFACTURED, NO_OF_OWNERS and DAYS_SINCE_REGISTERED have a negative relationship with PRICE.\n\ndf_second &lt;- df_numeric |&gt;\n  select(PRICE, MILEAGE_KM, CURB_WEIGHT_KG, OMV, NUM_PAST_OWNERS, COE_LISTED, ENGINE_CAPACITY_CC, DAYS_OF_COE_LEFT, FEATURE_COUNT, ACCESSORIES_COUNT, AGE_SINCE_MANUFACTURED, DAYS_SINCE_REGISTERED)\n\ncorrplot(cor(df_second), method = \"color\", type = \"lower\", tl.cex = 0.5, tl.col = \"black\", tl.srt = 45, addCoef.col = \"black\", number.cex = 0.6)"
  },
  {
    "objectID": "project_1.html#raster-plots-for-numerical-variables",
    "href": "project_1.html#raster-plots-for-numerical-variables",
    "title": "Big Data Analytics Group Project",
    "section": "Raster Plots for numerical variables",
    "text": "Raster Plots for numerical variables\n\ndf_second |&gt;\n  dbplot_raster(x = MILEAGE_KM, y = PRICE, fill = n(), resolution = 10) +\n  scale_x_continuous(\"Mileage in Thousands Kilometre\", \n                     breaks = seq(0, 2000000, by = 100000),\n                     labels = scales::comma_format(scale = 1e-3)) +\n  scale_y_continuous(\"Price in Millions\",\n                     breaks = seq(0, 3000000, by = 500000),\n                     labels = scales::comma_format(scale = 1e-6))\n\n\n\n\n\n\n\n# PRICE against NUM_PAST_OWNERS\ndf_second |&gt;\n  dbplot_raster(x = NUM_PAST_OWNERS, y = PRICE, fill = n(), resolution = 10) +\n  scale_x_continuous(\"Number of Owners\") +\n  scale_y_continuous(\"Price in Millions\",\n                     breaks = seq(0, 3000000, by = 500000),\n                     labels = scales::comma_format(scale = 1e-6))\n\n\n\n\n\n\n\n# PRICE against OMV\ndf_second |&gt;\n  dbplot_raster(x = OMV, y = PRICE, fill = n(), resolution = 10) +\n  scale_x_continuous(\"Open Market Value in Thousands\", \n                     breaks = seq(0, 2000000, by = 100000),\n                     labels = scales::comma_format(scale = 1e-3)) +\n  scale_y_continuous(\"Price in Millions\",\n                     breaks = seq(0, 3000000, by = 500000),\n                     labels = scales::comma_format(scale = 1e-6))\n\n\n\n\n\n\n\n# PRICE against COE_LISTED\ndf_second |&gt;\n  dbplot_raster(x = COE_LISTED, y = PRICE, fill = n(), resolution = 10) +\n  scale_x_continuous(\"COE Listed\") +\n  scale_y_continuous(\"Price in Millions\",\n                     breaks = seq(0, 3000000, by = 500000),\n                     labels = scales::comma_format(scale = 1e-6))\n\n\n\n\n\n\n\n# PRICE against ENGINE_CAPACITY_CC\ndf_second |&gt;\n  dbplot_raster(x = ENGINE_CAPACITY_CC, y = PRICE, fill = n(), resolution = 10) +\n  scale_x_continuous(\"Engine Capacity\") +\n  scale_y_continuous(\"Price in Millions\",\n                     breaks = seq(0, 3000000, by = 500000),\n                     labels = scales::comma_format(scale = 1e-6))\n\n\n\n\n\n\n\n# PRICE against CURB_WEIGHT_KG\ndf_second |&gt;\n  dbplot_raster(x = CURB_WEIGHT_KG, y = PRICE, fill = n(), resolution = 10) +\n  scale_x_continuous(\"Curb Weight in Kilograms\") +\n  scale_y_continuous(\"Price in Millions\",\n                     breaks = seq(0, 3000000, by = 500000),\n                     labels = scales::comma_format(scale = 1e-6))\n\n\n\n\n\n\n\n# PRICE against DAYS_OF_COE_LEFT\ndf_second |&gt;\n  dbplot_raster(x = DAYS_OF_COE_LEFT, y = PRICE, fill = n(), resolution = 10) +\n  scale_x_continuous(\"Days of COE Left\") +\n  scale_y_continuous(\"Price in Millions\",\n                     breaks = seq(0, 3000000, by = 500000),\n                     labels = scales::comma_format(scale = 1e-6))\n\n\n\n\n\n\n\n# PRICE against FEATURE_COUNT\ndf_second |&gt;\n  dbplot_raster(x = FEATURE_COUNT, y = PRICE, fill = n(), resolution = 10) +\n  scale_x_continuous(\"Number of Features\") +\n  scale_y_continuous(\"Price in Millions\",\n                     breaks = seq(0, 3000000, by = 500000),\n                     labels = scales::comma_format(scale = 1e-6))\n\n\n\n\n\n\n\n# PRICE against ACCESSORIES_COUNT\ndf_second |&gt;\n  dbplot_raster(x = ACCESSORIES_COUNT, y = PRICE, fill = n(), resolution = 10) +\n  scale_x_continuous(\"Number of Accessories\") +\n  scale_y_continuous(\"Price in Millions\",\n                     breaks = seq(0, 3000000, by = 500000),\n                     labels = scales::comma_format(scale = 1e-6))\n\n\n\n\n\n\n\n# PRICE against AGE_SINCE_MANUFACTURED\ndf_second |&gt;\n  dbplot_raster(x = AGE_SINCE_MANUFACTURED, y = PRICE, fill = n(), resolution = 10) +\n  scale_x_continuous(\"Car's age since manufactured\") +\n  scale_y_continuous(\"Price in Millions\",\n                     breaks = seq(0, 3000000, by = 500000),\n                     labels = scales::comma_format(scale = 1e-6))\n\n\n\n\n\n\n\n# PRICE against DAYS_SINCE_REGISTERED\ndf_second |&gt;\n  dbplot_raster(x = DAYS_SINCE_REGISTERED, y = PRICE, fill = n(), resolution = 10) +\n  scale_x_continuous(\"Number of days since car is registered\") +\n  scale_y_continuous(\"Price in Millions\",\n                     breaks = seq(0, 3000000, by = 500000),\n                     labels = scales::comma_format(scale = 1e-6))"
  },
  {
    "objectID": "project_1.html#bar-plots-for-character-variables",
    "href": "project_1.html#bar-plots-for-character-variables",
    "title": "Big Data Analytics Group Project",
    "section": "Bar Plots for character variables",
    "text": "Bar Plots for character variables\n\ndf_chr &lt;- df |&gt;\n  select(PRICE, TYPE, TRANSMISSION, BRAND_CATEGORY) |&gt;\n  collect()\n\n# PRICE against BRAND CATEGORY\nggplot(df_chr, aes(x = BRAND_CATEGORY, y = PRICE)) +\n  geom_bar(fill = \"orange\", stat = \"summary\", fun = \"mean\") +\n  labs(x = \"Category of Car Brands\", y = \"Average Price in Thousands\") +\n  scale_y_continuous(breaks = seq(0, 2000000, by = 100000),\n                     labels = scales::comma_format(scale = 1e-3))\n\n\n\n\n\n\n\n# PRICE against TYPE\nggplot(df_chr, aes(x = TYPE, y = PRICE)) +\n  geom_bar(fill = \"orange\", stat = \"summary\", fun = \"mean\") +\n  labs(x = \"Car Types\", y = \"Average Price in Thousands\") +\n  scale_y_continuous(breaks = seq(0, 2000000, by = 100000),\n                     labels = scales::comma_format(scale = 1e-3))\n\n\n\n\n\n\n\n# PRICE against TRANSMISSION\nggplot(df_chr, aes(x = TRANSMISSION, y = PRICE)) +\n  geom_bar(fill = \"orange\", stat = \"summary\", fun = \"mean\") +\n  labs(x = \"Car Transmission\", y = \"Average Price in Thousands\")"
  },
  {
    "objectID": "project_2.html#greenhouse-gas-emissions",
    "href": "project_2.html#greenhouse-gas-emissions",
    "title": "Time Series Data Analysis Project",
    "section": "Greenhouse gas emissions",
    "text": "Greenhouse gas emissions\n\n\n\n\n\n\n\n\n\nThere is an accelerating upward trend in greenhouse gas emissions, probably due to increasing human activities."
  },
  {
    "objectID": "project_2.html#temperature-anomalies",
    "href": "project_2.html#temperature-anomalies",
    "title": "Time Series Data Analysis Project",
    "section": "Temperature Anomalies",
    "text": "Temperature Anomalies\n\n\n\n\n\n\n\n\n\nThere is also an increasing upward trend in temperature anomalies. This can be attributed to the increase in greenhouse gas emissions above, further increasing the surface temperature of the Earth."
  },
  {
    "objectID": "project_2.html#mean-sea-level",
    "href": "project_2.html#mean-sea-level",
    "title": "Time Series Data Analysis Project",
    "section": "Mean Sea Level",
    "text": "Mean Sea Level\n\n\n\n\n\n\n\n\n\nThere is also an upward trend in mean sea level for the past 140 years. This is probably due to thermal expansion and melting of land-based ice, causing the increase in Earth‚Äôs temperature."
  },
  {
    "objectID": "project_2.html#greenhouse-gas-emissions-1",
    "href": "project_2.html#greenhouse-gas-emissions-1",
    "title": "Time Series Data Analysis Project",
    "section": "Greenhouse gas emissions",
    "text": "Greenhouse gas emissions\n\nACF/PACF\nWe use ACF/PACF to identify patterns and relationships between observations at different time lags. By examining the decay or cut-off patterns in the plots, we can determine the order of autoregressive (AR) and moving average (MA) components in models like ARIMA (Autoregressive Integrated Moving Average).\n\n\n\n\n\n\n\n\n\n4th lag has significant spike before ACF values drop within the confidence interval, indicating the number of terms in the MA model, which is MA(4).\n\n\n\n\n\n\n\n\n\n1 significant spike at lag 1 and PACF values fall sharply within the confidence interval after. This suggests an AR(1) model.\n\n\nBest model\nWe considered auto arima as it is convenient for automatic model selection as it automatically searches through a range of possible ARIMA model specifications to identify the best-fitting model based on selected criteria. We also did manual Arima modelling as we realise that auto Arima may not capture all relevant aspects of the data.\n\n\nSeries: ghg_ts \nARIMA(0,1,0) with drift \nBox Cox transformation: lambda= 0.0819848 \n\nCoefficients:\n       drift\n      0.1094\ns.e.  0.0127\n\nsigma^2 = 0.0228:  log likelihood = 66.6\nAIC=-129.2   AICc=-129.11   BIC=-123.31\n\nTraining set error measures:\n                    ME      RMSE       MAE        MPE     MAPE      MASE\nTraining set -10078753 541373695 330494607 -0.0168317 1.534341 0.6861922\n                   ACF1\nTraining set 0.03589983\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    Ljung-Box test\n\ndata:  Residuals from ARIMA(0,1,0) with drift\nQ* = 15.96, df = 10, p-value = 0.1008\n\nModel df: 0.   Total lags used: 10\n\n\n                       ME      RMSE       MAE         MPE     MAPE      MASE\nTraining set     92074.15 233395571 162055510 -0.06343501 1.586946 0.7469053\nTest set     247042413.33 553154516 424192168  1.56996909 2.874838 1.9550795\n                   ACF1 Theil's U\nTraining set -0.1017599        NA\nTest set      0.1356965 0.9364229\n\n\nWe conclude that the best ARIMA model is (0, 1, 0) as it has the lowest MAPE and AICc than our manual Arima models."
  },
  {
    "objectID": "project_2.html#temperature-anomalies-1",
    "href": "project_2.html#temperature-anomalies-1",
    "title": "Time Series Data Analysis Project",
    "section": "Temperature anomalies",
    "text": "Temperature anomalies\nWe do the same as above for the other variables.\n\nACF/PACF\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArima modelling\n\n\nWarning in guerrero(x, lower, upper): Guerrero's method for selecting a Box-Cox\nparameter (lambda) is given for strictly positive data.\n\n\nSeries: temp_ts \nARIMA(0,1,2) with drift \nBox Cox transformation: lambda= 1.253547 \n\nCoefficients:\n          ma1      ma2   drift\n      -0.3710  -0.2426  0.0046\ns.e.   0.0787   0.0737  0.0020\n\nsigma^2 = 0.003582:  log likelihood = 196.89\nAIC=-385.78   AICc=-385.48   BIC=-374.01\n\nTraining set error measures:\n                       ME       RMSE        MAE      MPE     MAPE      MASE\nTraining set -0.001869223 0.09302429 0.07689913 83.65737 126.8209 0.9288366\n                    ACF1\nTraining set -0.01905851\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    Ljung-Box test\n\ndata:  Residuals from ARIMA(0,1,2) with drift\nQ* = 8.806, df = 8, p-value = 0.3589\n\nModel df: 2.   Total lags used: 10\n\n\n                      ME       RMSE        MAE       MPE      MAPE      MASE\nTraining set 0.006552768 0.08995245 0.07695409 143.31483 215.40489 0.9565581\nTest set     0.011865395 0.11716223 0.10616997  96.09611  96.09611 1.3197186\n                    ACF1 Theil's U\nTraining set 0.005167274        NA\nTest set     0.577995268  1.299916\n\n\nWe conclude that the best model is ARIMA(0, 1, 2) as it has the lowest RMSE/MAPE than our manual Arima models."
  },
  {
    "objectID": "project_2.html#mean-sea-level-1",
    "href": "project_2.html#mean-sea-level-1",
    "title": "Time Series Data Analysis Project",
    "section": "Mean sea level",
    "text": "Mean sea level\n\n\nSeries: msl_ts \nARIMA(1,2,2) \nBox Cox transformation: lambda= 1.316535 \n\nCoefficients:\n         ar1      ma1     ma2\n      0.3567  -1.6722  0.6989\ns.e.  0.1957   0.1550  0.1519\n\nsigma^2 = 498.5:  log likelihood = -629.18\nAIC=1266.37   AICc=1266.67   BIC=1278.11\n\nTraining set error measures:\n                    ME     RMSE      MAE        MPE     MAPE      MASE\nTraining set 0.5148822 5.309408 4.030768 -0.7652999 6.692395 0.8568315\n                    ACF1\nTraining set -0.03735404\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    Ljung-Box test\n\ndata:  Residuals from ARIMA(0,1,2) with drift\nQ* = 8.806, df = 8, p-value = 0.3589\n\nModel df: 2.   Total lags used: 10\n\n\n                      ME     RMSE      MAE       MPE     MAPE      MASE\nTraining set -0.05074935 5.656688 4.242850 -4.298562 13.01068 0.8643036\nTest set      8.06832080 9.097511 8.104087  7.826382  7.86589 1.6508693\n                  ACF1 Theil's U\nTraining set 0.1679530        NA\nTest set     0.4945515  1.923896\n\n\nWe conclude that the best model is ARIMA(0, 1, 2)."
  },
  {
    "objectID": "project_2.html#arima-x",
    "href": "project_2.html#arima-x",
    "title": "Time Series Data Analysis Project",
    "section": "ARIMA-X",
    "text": "ARIMA-X\nWe also conducted ARIMA-X modelling as we wanted to leverage on the information provided by both the time series data as well as additional exogenous variables to improve forecasting accuracy and to understand the relationship between our target variable and each of the predictors.\n\nBetween 2 variables\n\nTemperature anomalies vs Greenhouse gas emissions\nBest model\n\n\nWarning in guerrero(x, lower, upper): Guerrero's method for selecting a Box-Cox\nparameter (lambda) is given for strictly positive data.\n\n\nSeries: data[, \"temp.anomalies\"] \nRegression with ARIMA(1,2,2) errors \nBox Cox transformation: lambda= 1.131028 \n\nCoefficients:\n\n\nWarning in sqrt(diag(x$var.coef)): NaNs produced\n\n\n         ar1      ma1     ma2  xreg\n      0.4628  -1.8547  0.8691     0\ns.e.  0.0766   0.0083  0.0026   NaN\n\nsigma^2 = 0.006754:  log likelihood = 149.63\nAIC=-289.26   AICc=-288.81   BIC=-274.58\n\nTraining set error measures:\n                     ME      RMSE        MAE      MPE     MAPE      MASE\nTraining set 0.01289628 0.1011968 0.08427159 76.74575 128.5643 0.9195055\n                   ACF1\nTraining set 0.03088324\n\n\n\n\n\n\n\n\n\nWe generated a few models and decided on ARIMA(1, 2, 2) as our best model, with the lowest AICc value and an improved forecast plot.\n\n\nMean sea level vs temperature anomalies\n\n\nWarning in guerrero(x, lower, upper): Guerrero's method for selecting a Box-Cox\nparameter (lambda) is given for strictly positive data.\n\n\nSeries: data[, \"mean.sea.level\"] \nRegression with ARIMA(0,1,1) errors \nBox Cox transformation: lambda= 0.7605642 \n\nCoefficients:\n          ma1   drift    xreg\n      -0.3113  0.6992  1.6075\ns.e.   0.0798  0.1177  1.6278\n\nsigma^2 = 4.098:  log likelihood = -295.92\nAIC=599.83   AICc=600.13   BIC=611.6\n\nTraining set error measures:\n                     ME     RMSE      MAE       MPE     MAPE      MASE\nTraining set -0.3419077 5.478202 4.227579 -2.295289 13.57332 0.8986682\n                     ACF1\nTraining set -0.008332118\n\n\n\n\n\n\n\n\n\n\n\nMean sea level vs greenhouse gas emissions\n\n\nWarning in guerrero(x, lower, upper): Guerrero's method for selecting a Box-Cox\nparameter (lambda) is given for strictly positive data.\n\n\nSeries: data[, \"mean.sea.level\"] \nRegression with ARIMA(1,2,2) errors \nBox Cox transformation: lambda= 0.7605642 \n\nCoefficients:\n\n\nWarning in sqrt(diag(x$var.coef)): NaNs produced\n\n\n         ar1      ma1     ma2  xreg\n      0.3391  -1.6974  0.7256     0\ns.e.  0.1241   0.0430  0.0448   NaN\n\nsigma^2 = 3.897:  log likelihood = -291.58\nAIC=593.17   AICc=593.62   BIC=607.84\n\nTraining set error measures:\n                    ME     RMSE      MAE       MPE     MAPE      MASE\nTraining set 0.3156484 5.314409 4.009108 -3.744495 12.89853 0.8522272\n                    ACF1\nTraining set 0.002902288\n\n\n\n\n\n\n\n\n\nWe ended up with ARIMA(1,2,2) being the best model, with the lowest AICc value.\n\n\n\nAmong the 3 variables\n\nMean sea level as Y variable\n\n\nSeries: data[\"mean.sea.level\"] \nRegression with ARIMA(0,1,2) errors \n\nCoefficients:\n          ma1      ma2   drift  ghg.emission  temp.anomalies\n      -0.4201  -0.1637  1.3287             0          5.0938\ns.e.   0.0836   0.0838  0.1010             0          4.3947\n\nsigma^2 = 28.68:  log likelihood = -431.2\nAIC=874.4   AICc=875.04   BIC=892.05\n\nTraining set error measures:\n                      ME     RMSE      MAE       MPE     MAPE      MASE\nTraining set -0.02364897 5.239836 4.072119 -5.896119 15.85623 0.8656217\n                    ACF1\nTraining set 0.005115978\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    Ljung-Box test\n\ndata:  Residuals from Regression with ARIMA(0,1,2) errors\nQ* = 4.2628, df = 8, p-value = 0.8327\n\nModel df: 2.   Total lags used: 10\n\n\n\n\n\n\n\n\n\n\n\nTemperature anomalies as Y variable\n\n\nSeries: data[\"temp.anomalies\"] \nRegression with ARIMA(2,0,3) errors \n\nCoefficients:\n         ar1     ar2     ma1      ma2      ma3  intercept  ghg.emission\n      0.0058  0.8720  0.6365  -0.5240  -0.3082    -0.0994             0\ns.e.  0.0732  0.0672  0.1047   0.0992   0.0839     0.2462             0\n      mean.sea.level\n              0.0017\ns.e.          0.0014\n\nsigma^2 = 0.01005:  log likelihood = 127.77\nAIC=-237.55   AICc=-236.17   BIC=-211.01\n\nTraining set error measures:\n                       ME       RMSE       MAE      MPE     MAPE      MASE\nTraining set -0.003360672 0.09738181 0.0804904 47.35752 95.14296 0.8782482\n                    ACF1\nTraining set -0.01695221\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    Ljung-Box test\n\ndata:  Residuals from Regression with ARIMA(2,0,3) errors\nQ* = 3.2539, df = 5, p-value = 0.6609\n\nModel df: 5.   Total lags used: 10"
  },
  {
    "objectID": "project_2.html#vector-autoregression-var",
    "href": "project_2.html#vector-autoregression-var",
    "title": "Time Series Data Analysis Project",
    "section": "Vector Autoregression (VAR)",
    "text": "Vector Autoregression (VAR)\nWe used VAR analysis to conduct a simultaneous time series forecast of 2 interdependent variables.\nDue to model complexity, we choose to study only the variables temperature anomalies and mean sea level.\n\nTemperature anomalies and mean sea level\n\n\nLoading required package: MASS\n\n\n\nAttaching package: 'MASS'\n\n\nThe following object is masked from 'package:dplyr':\n\n    select\n\n\nThe following objects are masked from 'package:fma':\n\n    cement, housing, petrol\n\n\nLoading required package: strucchange\n\n\nLoading required package: zoo\n\n\n\nAttaching package: 'zoo'\n\n\nThe following objects are masked from 'package:base':\n\n    as.Date, as.Date.numeric\n\n\nLoading required package: sandwich\n\n\nLoading required package: lmtest\n\n\n$selection\nAIC(n)  HQ(n)  SC(n) FPE(n) \n     4      2      1      4 \n\n$criteria\n                1          2          3          4          5          6\nAIC(n) -1.2194397 -1.3016037 -1.3123297 -1.3293649 -1.2882188 -1.2705232\nHQ(n)  -1.1659288 -1.2124189 -1.1874709 -1.1688322 -1.0920122 -1.0386427\nSC(n)  -1.0877513 -1.0821230 -1.0050567 -0.9342996 -0.8053613 -0.6998734\nFPE(n)  0.2954004  0.2721153  0.2692471  0.2647601  0.2759802  0.2810529\n                7          8          9         10\nAIC(n) -1.2262336 -1.1822807 -1.1627808 -1.1335708\nHQ(n)  -0.9586791 -0.8790523 -0.8238785 -0.7589945\nSC(n)  -0.5677914 -0.4360463 -0.3287541 -0.2117518\nFPE(n)  0.2939881  0.3074825  0.3139063  0.3236826\n\n\nSince SC shows 1, we start with VAR(1) and iterate until there is no longer any residual serial correlation.\nOur chosen VAR model\n\n\n\n    Portmanteau Test (asymptotic)\n\ndata:  Residuals of VAR object varmodel5\nChi-squared = 17.505, df = 20, p-value = 0.62\n\n\nIt has the highest p-value and residuals do not contain serial correlation\nOur plot"
  },
  {
    "objectID": "project_2.html#vector-error-correction-model",
    "href": "project_2.html#vector-error-correction-model",
    "title": "Time Series Data Analysis Project",
    "section": "Vector Error Correction Model",
    "text": "Vector Error Correction Model\nSpecial case of VAR, includes all the terms that are in the VAR and more.\n\nDetermining number of cointegrating relationships\n\nsummary(ca.jo(tempmsl))\n\n\n###################### \n# Johansen-Procedure # \n###################### \n\nTest type: maximal eigenvalue statistic (lambda max) , with linear trend \n\nEigenvalues (lambda):\n[1] 0.1222926 0.0258292\n\nValues of teststatistic and critical values of test:\n\n          test 10pct  5pct  1pct\nr &lt;= 1 |  3.64  6.50  8.18 11.65\nr = 0  | 18.13 12.91 14.90 19.19\n\nEigenvectors, normalised to first column:\n(These are the cointegration relations)\n\n                  mean.sea.level.l2 temp.anomalies.l2\nmean.sea.level.l2            1.0000            1.0000\ntemp.anomalies.l2         -199.2354          142.5095\n\nWeights W:\n(This is the loading matrix)\n\n                 mean.sea.level.l2 temp.anomalies.l2\nmean.sea.level.d       -0.02337127      0.0074556040\ntemp.anomalies.d        0.00130072      0.0000577494\n\n\n3.64 is smaller than all of the critical values, thus we conclude that there is 1 cointegrating relationship.\n\n\nDetermine number of lags\n\nVARselect(tempmsl)\n\n$selection\nAIC(n)  HQ(n)  SC(n) FPE(n) \n     4      2      1      4 \n\n$criteria\n                1          2          3          4          5          6\nAIC(n) -1.2194397 -1.3016037 -1.3123297 -1.3293649 -1.2882188 -1.2705232\nHQ(n)  -1.1659288 -1.2124189 -1.1874709 -1.1688322 -1.0920122 -1.0386427\nSC(n)  -1.0877513 -1.0821230 -1.0050567 -0.9342996 -0.8053613 -0.6998734\nFPE(n)  0.2954004  0.2721153  0.2692471  0.2647601  0.2759802  0.2810529\n                7          8          9         10\nAIC(n) -1.2262336 -1.1822807 -1.1627808 -1.1335708\nHQ(n)  -0.9586791 -0.8790523 -0.8238785 -0.7589945\nSC(n)  -0.5677914 -0.4360463 -0.3287541 -0.2117518\nFPE(n)  0.2939881  0.3074825  0.3139063  0.3236826\n\n\n\n\nBuilding VECM model\nBuild a VECM model with lag 1 and 1 cointegrating relationship\n\n\n\n\n\n\n\n\n\n\n\n                                ECT  Intercept mean.sea.level -1\nEquation mean.sea.level -0.02337127 0.05809115      -0.284712293\nEquation temp.anomalies  0.00130072 0.12966909      -0.001091056\n                        temp.anomalies -1\nEquation mean.sea.level        1.57031527\nEquation temp.anomalies       -0.08026665\n\n\n\n\nLjung-Box test\n\nresiduals &lt;- resid(vecmmodel)\n\nBox.test(residuals[,1], lag = 1, type = \"Ljung\") # mean sea level\n\n\n    Box-Ljung test\n\ndata:  residuals[, 1]\nX-squared = 0.16196, df = 1, p-value = 0.6874\n\nBox.test(residuals[,2], lag = 1, type = \"Ljung\") # temp anomalies\n\n\n    Box-Ljung test\n\ndata:  residuals[, 2]\nX-squared = 0.084895, df = 1, p-value = 0.7708\n\n\nSince p-values are more than 0.05, we fail to reject null hypothesis. This indicates that there is insufficient evidence to conclude that there is autocorrelation.\n\n\nPredict future values\n\npredict(vecmmodel, n.ahead=80)\n\n    mean.sea.level temp.anomalies\n142       67.85203      0.9567247\n143       70.16557      0.9291047\n144       72.20801      0.9089553\n145       74.19780      0.8963804\n146       76.10942      0.8891019\n147       77.97305      0.8858563\n148       79.79801      0.8856045\n149       81.59485      0.8875936\n150       83.37048      0.8912552\n151       85.13033      0.8961664\n152       86.87838      0.9020109\n153       88.61761      0.9085524\n154       90.35026      0.9156147\n155       92.07799      0.9230658\n156       93.80204      0.9308074\n157       95.52335      0.9387660\n158       97.24262      0.9468867\n159       98.96035      0.9551284\n160      100.67694      0.9634605\n161      102.39268      0.9718601\n162      104.10778      0.9803102\n163      105.82240      0.9887979\n164      107.53666      0.9973138\n165      109.25066      1.0058508\n166      110.96447      1.0144034\n167      112.67812      1.0229677\n168      114.39166      1.0315408\n169      116.10512      1.0401205\n170      117.81852      1.0487050\n171      119.53187      1.0572932\n172      121.24519      1.0658841\n173      122.95848      1.0744770\n174      124.67175      1.0830715\n175      126.38501      1.0916671\n176      128.09826      1.1002635\n177      129.81150      1.1088606\n178      131.52473      1.1174582\n179      133.23796      1.1260561\n180      134.95119      1.1346542\n181      136.66441      1.1432526\n182      138.37763      1.1518511\n183      140.09085      1.1604498\n184      141.80407      1.1690485\n185      143.51729      1.1776472\n186      145.23051      1.1862460\n187      146.94372      1.1948449\n188      148.65694      1.2034438\n189      150.37016      1.2120427\n190      152.08337      1.2206416\n191      153.79659      1.2292405\n192      155.50981      1.2378394\n193      157.22302      1.2464384\n194      158.93624      1.2550373\n195      160.64945      1.2636362\n196      162.36267      1.2722352\n197      164.07589      1.2808341\n198      165.78910      1.2894331\n199      167.50232      1.2980320\n200      169.21553      1.3066310\n201      170.92875      1.3152299\n202      172.64197      1.3238289\n203      174.35518      1.3324278\n204      176.06840      1.3410268\n205      177.78161      1.3496257\n206      179.49483      1.3582247\n207      181.20805      1.3668236\n208      182.92126      1.3754226\n209      184.63448      1.3840215\n210      186.34769      1.3926205\n211      188.06091      1.4012194\n212      189.77412      1.4098184\n213      191.48734      1.4184173\n214      193.20056      1.4270163\n215      194.91377      1.4356152\n216      196.62699      1.4442142\n217      198.34020      1.4528131\n218      200.05342      1.4614121\n219      201.76664      1.4700110\n220      203.47985      1.4786100\n221      205.19307      1.4872089"
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Qistina's Portfolio",
    "section": "",
    "text": "This section showcases the projects I‚Äôve worked on during my university years and beyond, including both group and individual projects.\n\n\n\n\nVisualising Singapore‚Äôs Public Transport Data\nThis is a self-project to visualise data in Python, focusing on plotting and analysing public transport data to gain insights.\n\n\n\nAnalysing and Modelling Resale Car Prices\nThis is a group project for Big Data Analytics Module, focusing on analysing resale car prices on SGCarMart. I am involved in visualising the data, carrying out exploratory data analysis and creating a shiny web app to allow users to predict resale car prices based on selected variables.\n\n\n\nExamining and Modelling Climate Change Data\nThis is a group project for Time Series Data Analysis Module, investigate the relationship between climate change and three variables by examining its patterns and trends. I am involved in most of the project, such as carrying out the tests, differencing, variable analysis and modelling.\n\n\n\nForecasting HDB Resale Prices\nThis is a group project for Visual Analytics & Business Intelligence Module, focusing on optimising HDB resale pricing and building a data-driven tool for buyers and sellers. I am involved in building the forecast chart for the prices based on various attributes as well as creating a public visualisation on Tableau Public."
  },
  {
    "objectID": "posts/index.html",
    "href": "posts/index.html",
    "title": "Welcome",
    "section": "",
    "text": "Hi, nice to meet you! üçÄ\nI‚Äôm Qistina, a final-year Economics and Data Science student at Singapore Management University.\nI‚Äôm passionate about using data to explore topics in environmental sustainability, space conservation, and animal welfare. Since I was young, I‚Äôve been drawn to protecting animals and the planet. Today, I aim to use my skills to contribute meaningfully to these causes.\nWhen I‚Äôm not coding or analyzing data, I enjoy spending time with my cats üêà, watching movies üçø, and wandering around Singapore.\nI‚Äôm currently open to data-related opportunities‚Äîwhether it‚Äôs in analytics, research, or sustainability.\nLet‚Äôs connect!"
  },
  {
    "objectID": "projects.html#group",
    "href": "projects.html#group",
    "title": "Qistina's Portfolio",
    "section": "Group",
    "text": "Group\n\n\n\n\nBig Data Analysis Project: Resale Car Prices\nThis is a group project for Big Data Analytics Module, focusing on analysing resale car prices on SGCarMart. My work involves visualising the data, carrying out exploratory data analysis and creating a shiny web app to allow users to predict resale car prices based on selected variables.\n\n\n\nTime Series Analysis Project: Climate Change Data\n\n\n\nVisual Analytics & Business Intelligence Project: HDB Resale Prices"
  },
  {
    "objectID": "project_4.html",
    "href": "project_4.html",
    "title": "Visualising Singapore‚Äôs Public Transport Data",
    "section": "",
    "text": "In this project, I aim to visualise Singapore‚Äôs public transport data to explore key trends, patterns, and insights that can inform decision-making and improve user understanding. The dataset was found here. The project involves first cleaning the data, followed by creating visualisations. This blog summarises my thought process, while the complete, runnable code and results are available here.\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt"
  },
  {
    "objectID": "project_4.html#reformat-dataset",
    "href": "project_4.html#reformat-dataset",
    "title": "Visualising Singapore‚Äôs Public Transport Data",
    "section": "Reformat dataset",
    "text": "Reformat dataset\nThe original dataset was structured with years as columns and variables as rows, so I transposed it to reformat the data for easier visualisation.\nmy_file = 'PublicTransportOperationAndRidershipAnnual.csv'\ndf = pd.read_csv(my_file) \n\ndf_transposed = df.set_index('DataSeries').transpose()\n\ndf_transposed = df.set_index('DataSeries').transpose().reset_index()\ndf_transposed = df_transposed.rename(columns={'index': 'year'})\nThe final dataset contains columns year, Rail Length, MRT Rail Length, LRT Rail Length, MRT km operated, LRT km operated, Average daily ridership (MRT), Average daily ridership (LRT), Average daily ridership (Bus) and Average Daily Trip - Point-To-Point (P2P) Transport (Taxis And Private Hire Cars)."
  },
  {
    "objectID": "project_4.html#renaming-columns",
    "href": "project_4.html#renaming-columns",
    "title": "Visualising Singapore‚Äôs Public Transport Data",
    "section": "Renaming columns",
    "text": "Renaming columns\nI renamed the columns using lowercase letters and underscores for consistency and ease of use in code.\ndf_transposed = df_transposed.rename(columns = {'Rail Length': 'total_rail_length',\n                                                '    Mass Rapid Transit (MRT)': 'mrt_rail_length',\n                                                '    Light Rail Transit (LRT)': 'lrt_rail_length',\n                                                'MRT km Operated': 'mrt_km_operated',\n                                                'LRT km Operated': 'lrt_km_operated',\n                                                'Average Daily Ridership - MRT': 'mrt_avg_daily_ridership',\n                                                'Average Daily Ridership - LRT': 'lrt_avg_daily_ridership',\n                                                'Average Daily Ridership - Public Bus': 'bus_avg_daily_ridership',\n                                                'Average Daily Trip - Point-To-Point (P2P) Transport (Taxis And Private Hire Cars)': 'p2p_avg_daily_trip'})"
  },
  {
    "objectID": "project_4.html#fill-in-missing-values",
    "href": "project_4.html#fill-in-missing-values",
    "title": "Visualising Singapore‚Äôs Public Transport Data",
    "section": "Fill in missing values",
    "text": "Fill in missing values\nThe dataset uses ‚Äòna‚Äô to denote missing values, which I replaced with NaN for compatibility with pandas dataframes. For missing LRT rail length data, I replaced it with 0, as the sum of MRT and LRT rail lengths must match the total rail length.\ndf_transposed = df_transposed.replace(['na'], pd.NA)\n\nlrt_missing = df_transposed['lrt_rail_length'].isna()\n\ndf_transposed.loc[lrt_missing, 'lrt_rail_length'] = 0"
  },
  {
    "objectID": "project_4.html#converting-datatypes-to-numeric",
    "href": "project_4.html#converting-datatypes-to-numeric",
    "title": "Visualising Singapore‚Äôs Public Transport Data",
    "section": "Converting datatypes to numeric",
    "text": "Converting datatypes to numeric\nThe variables are of object type, so I converted them to numeric values.\ndf_transposed = df_transposed.apply(pd.to_numeric, errors='coerce')"
  },
  {
    "objectID": "project_4.html#dropping-unnecessary-columns",
    "href": "project_4.html#dropping-unnecessary-columns",
    "title": "Visualising Singapore‚Äôs Public Transport Data",
    "section": "Dropping unnecessary columns",
    "text": "Dropping unnecessary columns\nSince the analysis focuses only on public transport, I dropped the p2p_avg_daily_trip column. Additionally, the dataset has significant gaps for years before 2009, so I removed those rows to maintain data quality.\ndf_transposed = df_transposed.drop(columns = ['p2p_avg_daily_trip'])\n\ndf_transposed = df_transposed[0:15]"
  },
  {
    "objectID": "project_4.html#plotting-rail-length-against-year",
    "href": "project_4.html#plotting-rail-length-against-year",
    "title": "Visualising Singapore‚Äôs Public Transport Data",
    "section": "Plotting rail length against year",
    "text": "Plotting rail length against year\nplt.style.use('ggplot')\n\ndf_sorted = df_transposed.sort_values(by='year')\n\nplt.plot(df_sorted['year'], df_sorted['total_rail_length'], marker='o', label=\"Total\")\nplt.plot(df_sorted['year'], df_sorted['mrt_rail_length'], marker='o', label=\"MRT\")\nplt.plot(df_sorted['year'], df_sorted['lrt_rail_length'], marker='o', label=\"LRT\")\n\nplt.xlabel('Year')\nplt.ylabel('Total Rail Length (in kilometres)')\nplt.title('Total Rail Length vs Year')\nplt.xticks(fontsize=8)\nplt.yticks(fontsize=10)\n\nplt.legend(loc=(1.02, 0)) \n\nplt.tight_layout() \n\nplt.show()"
  },
  {
    "objectID": "project_4.html#plotting-riderships-against-year",
    "href": "project_4.html#plotting-riderships-against-year",
    "title": "Visualising Singapore‚Äôs Public Transport Data",
    "section": "Plotting riderships against year",
    "text": "Plotting riderships against year\nplt.plot(df_sorted['year'], df_sorted['mrt_avg_daily_ridership'], marker='o', label='MRT')\nplt.plot(df_sorted['year'], df_sorted['lrt_avg_daily_ridership'], marker='o', label='LRT')\nplt.plot(df_sorted['year'], df_sorted['bus_avg_daily_ridership'], marker='o', label='Bus')\n\nplt.xlabel('Year')\nplt.ylabel('Average Daily Ridership (in thousands)')\nplt.title('Average Daily Ridership vs Year')\n\nplt.xticks(fontsize=8)\nplt.yticks(fontsize=10)\n\nplt.legend(loc=(1.02, 0)) \n\nplt.tight_layout() \n\nplt.show()"
  },
  {
    "objectID": "blog_1.html",
    "href": "blog_1.html",
    "title": "British Pound(ed) üî®",
    "section": "",
    "text": "This is a blog post based on my class presentation for Microeconomics 1 (ECON111) during the fall semester of 2022. The task was to select a news article related to a microeconomic concept, write an analysis linking theory to the real-world example, and present our findings in class.\nI chose the article ‚ÄòWhy is the British Pound getting pounded?‚Äô by Paul Krugman from The New York Times.\n\nTax & Spending Plan\nSpending Plan\nThe UK government planned to increase spendings to help consumers and businesses hit with high energy prices due to Putin‚Äôs de facto natural gas embargo.\nA cap of 60 billion pounds on energy bills was estimated for the first six months.\nTax Plan\nInstead of raising taxes to cover the expense of the spending plan, Liz Truss, ex-Prime Minister of UK, announced tax cuts.\nThis disproportionately favored high wage earners, as it abolished top rate of 45% of income tax on those earning more than 150,000 pounds while cutting basic rate for lower earners and house purchases.\nThis plan was reminiscent of Reaganomics, as it focused on tax cuts aimed at stimulating growth through supply-side measures.\n\n\nReagonomics\nIn 1980s, Ronald Reagan (40th US President), implemented tax cuts while increasing military spending. This caused the dollar to rise against other currencies and the US economy to grow faster.\nThis was not the case with the British Pound.\n\n\nThe fall of British Pound\nInstead of the pound getting stronger, it plunged.\nDue to the pound plunging, investors hoped that the Bank of England would implement an emergency interest rate hike to stabilise the pound. The Bank was hesitant to do so, and chose to wait instead.\n\n\nConflict\nAt the time of the announcement, Britain was already facing high inflation so the Bank of England increased interest rates seven times to curb the surging prices and dampen economic activity.\nHowever, the government announced policies that were trying to drive up the economy. By widely cutting taxes and stoking the housing market, it could fuel higher inflation.\n\n\n1992 Sterling Crisis\nTo understand why the Bank is reluctant to raise interest rates, Paul examined the 1992 sterling crisis.\n\nUnemployment Rate\n\nIn 2022, UK‚Äôs unemployment rate recently fell to its lowest since 1974. An increase in the interest rate may heighten the unemployment rate again, and efforts to reduce unemployment will be in vain\n\nMortgage\n\nBritish homeowners have floating rate mortgages, where the interest rates vary with the market. Higher interest rates would increase their mortgages, causing financial burden as homeowners have to pay hundreds of thousands pounds extra per year.\n\n\nConclusion\nStudying this event through a microeconomic lens reveals how expectations, incentives, and policy signals shape real-world outcomes. Though the UK‚Äôs plan resembled Reagan-era thinking, markets responded very differently, which is a stark example of how context matters in economics."
  },
  {
    "objectID": "blogs.html",
    "href": "blogs.html",
    "title": "Blog",
    "section": "",
    "text": "This blog is a space where I simplify and reflect on concepts I‚Äôve encountered, whether in class, articles, or current events. This allows me to process and make sense of ideas, especially in economics and data.\n\n\n\nUnderstanding IPOs through Microsoft Stock Growth\nThis post breaks down Microsoft‚Äôs 1986 IPO and traces its stock price growth to today. I explain key IPO concepts like stock splits and dividends, show how to analyze stock data with Python, and share what I‚Äôve learned as someone starting out in finance and data analysis.\n\n\nThe 2022 British Pound Crash\nThis post reflects on Paul Krugman‚Äôs New York Times commentary on the 2022 British Pound crisis. I summarize his main arguments, connect them to Reagan-era economic theory, and share what I took away from the analysis as a student of economics."
  },
  {
    "objectID": "blog_2.html",
    "href": "blog_2.html",
    "title": "Understanding IPOs through Microsoft Stock Growth",
    "section": "",
    "text": "Initial Public Offering, or better known as IPO, is the process where a private company sells shares of its stocks to the public on a stock exchange. By transitioning from private to public ownership, it provides the company with an opportunity to raise capital to fund growth, or pay off debt. Going public also allows early investors of the company to sell some or all of their shares in the company."
  },
  {
    "objectID": "blog_2.html#introduction",
    "href": "blog_2.html#introduction",
    "title": "Understanding IPOs through Microsoft Stock Growth",
    "section": "",
    "text": "Initial Public Offering, or better known as IPO, is the process where a private company sells shares of its stocks to the public on a stock exchange. By transitioning from private to public ownership, it provides the company with an opportunity to raise capital to fund growth, or pay off debt. Going public also allows early investors of the company to sell some or all of their shares in the company."
  },
  {
    "objectID": "blog_2.html#ipo-process",
    "href": "blog_2.html#ipo-process",
    "title": "Understanding IPOs through Microsoft Stock Growth",
    "section": "IPO Process",
    "text": "IPO Process\nA company that is interested in an IPO would either approach underwriters privately to see who‚Äôs interested in handling the IPO, or make a public announcement to generate interest. The underwriters chosen by the company would then lead the IPO process and handle aspects such as due diligence, filing, marketing, etc."
  },
  {
    "objectID": "blog_2.html#example-microsoft",
    "href": "blog_2.html#example-microsoft",
    "title": "Understanding IPOs through Microsoft Stock Growth",
    "section": "Example: Microsoft",
    "text": "Example: Microsoft\nIn 1986, Microsoft decided to pursue a public stock offering. Microsoft chose world renowned investment banking company Goldman Sachs as their underwriters, as well as Alex. Brown & Sons. Initially, the underwriters chose an offer price of US$17-20 per share but Bill Gates suggested a lower range of US$16-19. After conducting a roadshow to interact with potential investors, Microsoft realised that the interest was so strong that Goldman Sachs suggested Microsoft to increase the price range and the number of shares to be sold. They then decided to set the IPO price at US$21 and offered 2.5 million shares. The offering price increased to $35.50 at the end of the day with an additional 295,000 shares sold. Microsoft‚Äôs IPO was dubbed IPO of the year, generating a whopping US$61 million."
  },
  {
    "objectID": "blog_2.html#tracking-microsofts-stock-price",
    "href": "blog_2.html#tracking-microsofts-stock-price",
    "title": "Understanding IPOs through Microsoft Stock Growth",
    "section": "Tracking Microsoft‚Äôs Stock Price",
    "text": "Tracking Microsoft‚Äôs Stock Price\nThat was in 1986, almost 40 years ago. What is the price of a Microsoft share now? Let‚Äôs find out through a small coding project.\nimport yfinance as yf\nimport matplotlib.pyplot as plt\n\n# Latest stock price\nticker = yf.Ticker(\"MSFT\")  \ncurrent_price = ticker.fast_info[\"lastPrice\"]\nprint(\"Current stock price:\", current_price)\n\n# Plot stock prices since IPO\nmsft = yf.Ticker(\"MSFT\")\ndata = msft.history(period=\"max\")\n\nplt.figure(figsize=(12, 6))\nplt.plot(data.index, data[\"Close\"], label=\"MSFT Closing Price\", color='green')\n\nplt.title(\"Microsoft (MSFT) Stock Price Since IPO (log scale)\")\nplt.xlabel(\"Year\")\nplt.ylabel(\"Price in USD\")\nplt.yscale(\"log\")\nplt.grid(True)\nplt.legend()\nplt.tight_layout()\nplt.show()\nAs of 1 July:\nCurrent Price (1 July 2025): $497\n\nIt may not seem like much at first, but that is an increase of about 2,271%. If someone had bought $1000 worth of stocks during the IPO, their investment would be worth over $23,000 today! Of course, that‚Äôs not counting stock splits or dividends, which would increase their investment even more.\n\nWhat‚Äôs stock splits / dividends?\nStock splits is when a company divides its stock into multiple shares, lowering the price of each share without changing the company‚Äôs market value. Dividends is the sum of money paid by a company to its shareholders out of its profits. By also considering Microsoft‚Äôs stock splits and dividends over the years, the total return on an initial investment is even higher."
  },
  {
    "objectID": "blog_2.html#conclusion",
    "href": "blog_2.html#conclusion",
    "title": "Understanding IPOs through Microsoft Stock Growth",
    "section": "Conclusion",
    "text": "Conclusion\nFrom Microsoft‚Äôs humble IPO in 1986 to becoming one of the world‚Äôs most valuable companies, this story is a powerful reminder of how much can change over time. Rising from a $21 IPO price to nearly $500 today, Microsoft‚Äôs growth highlights how the stock market can reward patience, innovation, and long-term thinking. As someone starting out in finance, exploring IPOs and company performance has been a great way to deepen my understanding of the markets, and combining that with coding has helped me uncover insights through data in an even more meaningful way :)\nYou can also track a stock price history using my code here!"
  }
]